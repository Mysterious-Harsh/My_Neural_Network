{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7148c80d-d3a4-49bc-a720-c101861beefa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "import numpy as np\n",
    "from nnfs.datasets import spiral_data, sine_data\n",
    "from matplotlib import ticker, cm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5a383f4-8a30-4579-bd2c-ba8976ce3602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "plt.ion()\n",
    "# plt.rcParams[\"animation.html\"]=\"jshtml\"\n",
    "nnfs.init()\n",
    "# plt.style.use(\"seaborn-v0_8-bright\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c569bea9-b397-4fe9-94d3-a88c846fa917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, epochs):\n",
    "        self.dense_layers = []\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = None\n",
    "        self.flag = False\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def add_dense(\n",
    "        self,\n",
    "        n_inputs,\n",
    "        n_neurons,\n",
    "        activation,\n",
    "        L1_weight_regularizer=0,\n",
    "        L2_weight_regularizer=0,\n",
    "        L1_bias_regularizer=0,\n",
    "        L2_bias_regularizer=0,\n",
    "    ):\n",
    "        dense = Dense(\n",
    "            n_inputs,\n",
    "            n_neurons,\n",
    "            activation,\n",
    "            L1_weight_regularizer,\n",
    "            L2_weight_regularizer,\n",
    "            L1_bias_regularizer,\n",
    "            L2_bias_regularizer,\n",
    "        )\n",
    "        self.dense_layers.append(dense)\n",
    "\n",
    "    def add_dropout(self, rate=0.1):\n",
    "        self.dense_layers.append(Dropout(rate))\n",
    "\n",
    "    def add_loss(self, loss_function):\n",
    "\n",
    "        if (\n",
    "            loss_function == Categorical_Cross_Entropy\n",
    "            and self.dense_layers[-1].activation.__class__.__name__ == \"Softmax\"\n",
    "        ):\n",
    "            self.dense_layers[-1].activation = Softmax_Categorical_Cross_Entropy_Loss()\n",
    "            self.flag = True\n",
    "            # print(self.dense_layers)\n",
    "        self.loss_function = loss_function()\n",
    "\n",
    "    def add_optimizer(self, optimizer, **params):\n",
    "        self.optimizer = optimizer(**params)\n",
    "\n",
    "    def fit(self, X, y,fig,ax):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        accuracy_precision = np.std(self.y) / 250\n",
    "        \n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            layer_output = self.X\n",
    "            regularization_loss = 0\n",
    "            for layer in self.dense_layers:\n",
    "                layer_output = layer.forward(layer_output)\n",
    "                if layer.TYPE == \"dense\":\n",
    "                    regularization_loss += layer.regularization()\n",
    "\n",
    "            data_loss = self.loss_function.calculate(layer_output, self.y)\n",
    "            loss = data_loss + regularization_loss\n",
    "\n",
    "            if self.loss_function.__class__.__name__ == \"Categorical_Cross_Entropy\":\n",
    "                if len(self.y.shape) == 2:\n",
    "                    self.y = np.argmax(self.y, axis=1)\n",
    "                predictions = np.argmax(layer_output, axis=1)\n",
    "                accuracy = np.mean(predictions == self.y)\n",
    "            elif self.loss_function.__class__.__name__ == \"Binary_Cross_Entropy\":\n",
    "                predictions = (layer_output > 0.5) * 1\n",
    "                accuracy = np.mean(predictions == self.y)\n",
    "            elif self.loss_function.__class__.__name__ in [\"MAR\", \"MSE\"]:\n",
    "                predictions = layer_output\n",
    "                accuracy = np.mean(\n",
    "                    np.absolute(predictions - self.y) < accuracy_precision\n",
    "                )\n",
    "                if not epoch % 100:\n",
    "                    ax.plot(self.X, self.y)\n",
    "                    ax.plot(self.X, predictions)\n",
    "                    fig.canvas.draw()\n",
    "                    fig.canvas.flush_events()\n",
    "                    time.sleep(0.1)\n",
    "\n",
    "            if not epoch % 100:\n",
    "                print(\n",
    "                    f\"Epochs: {epoch}, Acc: {accuracy}, Loss: {loss}, Data_Loss:\"\n",
    "                    f\" {data_loss}, Reg_Loss: {regularization_loss}, Lr:\"\n",
    "                    f\" {self.optimizer.current_learning_rate}\"\n",
    "                )\n",
    "            reversed_dense_layer = self.dense_layers.copy()\n",
    "            reversed_dense_layer.reverse()\n",
    "\n",
    "            if self.flag:\n",
    "\n",
    "                dvalues = reversed_dense_layer[0].backward(layer_output, self.y)\n",
    "\n",
    "                n = 1\n",
    "            else:\n",
    "\n",
    "                dvalues = self.loss_function.backward(layer_output, self.y)\n",
    "\n",
    "                n = 0\n",
    "            for layer in reversed_dense_layer[n:]:\n",
    "                dvalues = layer.backward(dvalues)\n",
    "            self.optimizer.pre_update_params()\n",
    "            for layer in self.dense_layers:\n",
    "                if layer.TYPE == \"dense\":\n",
    "\n",
    "                    self.optimizer.update_params(layer)\n",
    "            self.optimizer.post_update_params()\n",
    "\n",
    "    def show_boundary(self):\n",
    "        x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "        y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)\n",
    "        )\n",
    "        layer_output = np.c_[xx.ravel(), yy.ravel()]\n",
    "        for layer in self.dense_layers:\n",
    "            layer_output = layer.forward(layer_output)\n",
    "        print(layer_output.shape[1] == 1)\n",
    "        if layer_output.shape[1] == 1:\n",
    "            Z = (layer_output > 0.5) * 1\n",
    "            Z = Z.reshape(xx.shape)\n",
    "        else:\n",
    "            Z = np.argmax(layer_output, axis=1).reshape(xx.shape)\n",
    "\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # plt.contourf(xx, yy, Z, alpha=0.2)\n",
    "        plt.set_cmap(plt.cm.brg)\n",
    "        plt.pcolormesh(xx, yy, Z, alpha=0.2, shading=\"gouraud\", snap=True)\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, c=y, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a436bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate) -> None:\n",
    "        self.rate = 1 - rate\n",
    "        self.TYPE = \"dropout\"\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.binary_mask = (\n",
    "            np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        )\n",
    "        self.output = self.inputs * self.binary_mask\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "        return self.dinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c037fe5-00d1-4064-b7ef-fbfc2cd63fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_inputs,\n",
    "        n_neurons,\n",
    "        activation,\n",
    "        L1_weight_regularizer=0,\n",
    "        L2_weight_regularizer=0,\n",
    "        L1_bias_regularizer=0,\n",
    "        L2_bias_regularizer=0,\n",
    "    ):\n",
    "        self.TYPE = \"dense\"\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.L1_weight_regularizer = L1_weight_regularizer\n",
    "        self.L2_weight_regularizer = L2_weight_regularizer\n",
    "        self.L1_bias_regularizer = L1_bias_regularizer\n",
    "        self.L2_bias_regularizer = L2_bias_regularizer\n",
    "        self.activation = activation()\n",
    "\n",
    "    def forward(self, inputs, *args):\n",
    "        self.inputs = inputs\n",
    "        self.output = self.activation.forward(\n",
    "            np.dot(self.inputs, self.weights) + self.biases, *args\n",
    "        )\n",
    "        return self.output\n",
    "\n",
    "    def regularization(self):\n",
    "        regularization = 0\n",
    "        if self.L1_weight_regularizer > 0:\n",
    "            regularization += self.L1_weight_regularizer * np.sum(np.abs(self.weights))\n",
    "        if self.L2_weight_regularizer > 0:\n",
    "            regularization += self.L2_weight_regularizer * np.sum(self.weights**2)\n",
    "        if self.L1_bias_regularizer > 0:\n",
    "            regularization += self.L1_bias_regularizer * np.sum(np.abs(self.biases))\n",
    "        if self.L2_bias_regularizer > 0:\n",
    "            regularization += self.L2_bias_regularizer * np.sum(self.biases**2)\n",
    "        return regularization\n",
    "\n",
    "    # dvalues is derivative we got from previous neurons\n",
    "    def backward(self, dvalues, *args):\n",
    "        dvalues = self.activation.backward(dvalues, *args)\n",
    "\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        if self.L1_weight_regularizer > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.L1_weight_regularizer * dL1\n",
    "\n",
    "        if self.L2_weight_regularizer > 0:\n",
    "            self.dweights += 2 * self.L2_weight_regularizer * self.weights\n",
    "\n",
    "        if self.L1_bias_regularizer > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.L1_bias_regularizer * dL1\n",
    "\n",
    "        if self.L2_bias_regularizer > 0:\n",
    "            self.dbiases += 2 * self.L2_bias_regularizer * self.biases\n",
    "\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        return self.dinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4af9752b-e6d9-4998-b051-763c84d8e3c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "\n",
    "    def forward(self, inputs, *args):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues, *args):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "\n",
    "    def forward(self, inputs, *args):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(\n",
    "            inputs - np.max(inputs, axis=1, keepdims=True), dtype=np.double\n",
    "        )\n",
    "        # if not exp_values.all():\n",
    "        #     print(exp_values)\n",
    "        #     input()\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues, *args):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        for index, (single_output, single_dvalues) in enumerate(\n",
    "            zip(self.output, dvalues)\n",
    "        ):\n",
    "            single_output = single_output.reshape(1, -1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(\n",
    "                single_output.T, single_output\n",
    "            )\n",
    "\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        return self.output\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        return self.dinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29f40030-f316-4a0a-a8ef-6beaff1bfaaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Loss:\n",
    "\n",
    "    def calculate(self, y_pred, y_actual, *args):\n",
    "        sample_losses = self.forward(y_pred, y_actual)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "class Binary_Cross_Entropy(Loss):\n",
    "    def forward(self, y_pred, y_actual):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(\n",
    "            y_actual * np.log(y_pred_clipped)\n",
    "            + (1 - y_actual) * np.log(1 - y_pred_clipped)\n",
    "        )\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        return sample_losses\n",
    "\n",
    "    def backward(self, dvalues, y_actual):\n",
    "        samples = len(dvalues)\n",
    "        output = len(dvalues[0])\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        self.dinputs = (\n",
    "            -(y_actual / clipped_dvalues - (1 - y_actual) / (1 - clipped_dvalues))\n",
    "            / output\n",
    "        )\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "class Categorical_Cross_Entropy(Loss):\n",
    "\n",
    "    def forward(self, y_pred, y_actual, *args):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        correct_confidences = []\n",
    "        if len(y_actual.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_actual]\n",
    "        elif len(y_actual.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_actual, axis=1)\n",
    "        negative_log = -np.log(correct_confidences)\n",
    "        return negative_log\n",
    "\n",
    "    def backward(self, dvalues, y_true, *args):\n",
    "\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        # dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        # if not dvalues.all():\n",
    "        #     print(dvalues)\n",
    "        #     input()\n",
    "        # print(np.where(dvalues == 0))\n",
    "        self.dinputs = -y_true / dvalues\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "class Softmax_Categorical_Cross_Entropy_Loss:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = Softmax()\n",
    "        self.loss = Categorical_Cross_Entropy()\n",
    "\n",
    "    def forward(self, inputs, *args):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        # return self.loss.calculate(self.output, y_actual)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dvalues, *args):\n",
    "        y_actual = args[0]\n",
    "        samples = len(dvalues)\n",
    "        if len(y_actual.shape) == 2:\n",
    "            y_actual = np.argmax(y_actual, axis=1)\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples), y_actual] -= 1\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "class MSE(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred) ** 2, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        return self.dinputs\n",
    "\n",
    "\n",
    "class MAR(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        return sample_losses\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        return self.dinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13061466-3939-4c76-b956-1bac6633d41c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "\n",
    "    def __init__(self, learning_rate=1, decay=0.0, momentum=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.momentum = momentum\n",
    "        self.iterations = 0\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            weight_update = (\n",
    "                self.momentum * layer.weight_momentums\n",
    "                - self.current_learning_rate * layer.dweights\n",
    "            )\n",
    "            layer.weight_momentums = weight_update\n",
    "\n",
    "            bias_update = (\n",
    "                self.momentum * layer.bias_momentums\n",
    "                - self.current_learning_rate * layer.dbiases\n",
    "            )\n",
    "            layer.bias_momentums = bias_update\n",
    "        else:\n",
    "            weight_update = -self.current_learning_rate * layer.dweights\n",
    "            bias_update = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        layer.weights += weight_update\n",
    "        layer.biases += bias_update\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "\n",
    "    def __init__(self, learning_rate=0.1, decay=0.0, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = 0\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        layer.weights += (-self.current_learning_rate * layer.dweights) / (\n",
    "            np.sqrt(layer.weight_cache) + self.epsilon\n",
    "        )\n",
    "        layer.biases += (-self.current_learning_rate * layer.dbiases) / (\n",
    "            np.sqrt(layer.bias_cache) + self.epsilon\n",
    "        )\n",
    "        # print(self.current_learning_rate)\n",
    "        # print(self.iterations)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "class RMSProp:\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, decay=0.0, epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 1\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        layer.weight_cache = (\n",
    "            self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        )\n",
    "        layer.bias_cache = (\n",
    "            self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "        )\n",
    "        layer.weights += (-self.current_learning_rate * layer.dweights) / (\n",
    "            np.sqrt(layer.weight_cache) + self.epsilon\n",
    "        )\n",
    "        layer.biases += (-self.current_learning_rate * layer.dbiases) / (\n",
    "            np.sqrt(layer.bias_cache) + self.epsilon\n",
    "        )\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "class Adam:\n",
    "\n",
    "    def __init__(\n",
    "        self, learning_rate=0.001, decay=0.0, epsilon=1e-7, beta_1=0.9, beta_2=0.999\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 1\n",
    "        self.epsilon = 1e-7\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = (\n",
    "            self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        )\n",
    "        layer.bias_momentums = (\n",
    "            self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        )\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / (\n",
    "            1 - self.beta_1 ** (self.iterations)\n",
    "        )\n",
    "        bias_momentums_corrected = layer.bias_momentums / (\n",
    "            1 - self.beta_1 ** (self.iterations)\n",
    "        )\n",
    "\n",
    "        layer.weight_cache = (\n",
    "            self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        )\n",
    "        layer.bias_cache = (\n",
    "            self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        )\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / (\n",
    "            1 - self.beta_2 ** (self.iterations)\n",
    "        )\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations))\n",
    "\n",
    "        layer.weights += (\n",
    "            -self.current_learning_rate\n",
    "            * weight_momentums_corrected\n",
    "            / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        )\n",
    "        layer.biases += (\n",
    "            -self.current_learning_rate\n",
    "            * bias_momentums_corrected\n",
    "            / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "        )\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791429c-fa14-47b4-81ca-b49ee4120d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T02:16:16.432310Z",
     "iopub.status.busy": "2023-05-30T02:16:16.431651Z",
     "iopub.status.idle": "2023-05-30T02:16:16.443234Z",
     "shell.execute_reply": "2023-05-30T02:16:16.442299Z",
     "shell.execute_reply.started": "2023-05-30T02:16:16.432258Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=500, classes=2)\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4315f-b425-45d7-8d9a-8cf43e6c768b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T02:16:17.293961Z",
     "iopub.status.busy": "2023-05-30T02:16:17.293744Z",
     "iopub.status.idle": "2023-05-30T02:16:17.383337Z",
     "shell.execute_reply": "2023-05-30T02:16:17.383021Z",
     "shell.execute_reply.started": "2023-05-30T02:16:17.293938Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap=\"brg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf789760-3233-48a6-8e9a-d5b822922357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T02:16:21.082724Z",
     "iopub.status.busy": "2023-05-30T02:16:21.082055Z",
     "iopub.status.idle": "2023-05-30T02:18:55.422337Z",
     "shell.execute_reply": "2023-05-30T02:18:55.420573Z",
     "shell.execute_reply.started": "2023-05-30T02:16:21.082672Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(1000)\n",
    "model.add_dense(2, 512, ReLU, L2_weight_regularizer=5e-8, L2_bias_regularizer=5e-8)\n",
    "# model.add_dropout()\n",
    "model.add_dense(512, 1, Sigmoid)\n",
    "model.add_loss(Binary_Cross_Entropy)\n",
    "model.add_optimizer(Adam, learning_rate=0.02, decay=5e-7)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08640cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_boundary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5523eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(12000)\n",
    "model.add_dense(2, 124, ReLU, L2_weight_regularizer=5e-8, L2_bias_regularizer=5e-8)\n",
    "# model.add_dropout()\n",
    "model.add_dense(124, 2, Softmax)\n",
    "model.add_loss(Categorical_Cross_Entropy)\n",
    "model.add_optimizer(Adam, learning_rate=0.02, decay=5e-7)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677cc0bb-239f-4c35-a455-0d0be215327c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T02:18:57.715381Z",
     "iopub.status.busy": "2023-05-30T02:18:57.714890Z",
     "iopub.status.idle": "2023-05-30T02:18:58.085931Z",
     "shell.execute_reply": "2023-05-30T02:18:58.079910Z",
     "shell.execute_reply.started": "2023-05-30T02:18:57.715327Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.show_boundary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ed32349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f2c4c9bccd4e0a8b6aca5a28fa6b36",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=640.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0, Acc: 0.002, Loss: 0.49963982780659333, Data_Loss: 0.49963982712463223, Reg_Loss: 6.819610987029989e-10, Lr: 0.005\n",
      "Epochs: 100, Acc: 0.005, Loss: 0.15933748084505786, Data_Loss: 0.15933733329228963, Reg_Loss: 1.4755276822597807e-07, Lr: 0.004545454545454545\n",
      "Epochs: 200, Acc: 0.004, Loss: 0.15596068974361718, Data_Loss: 0.15596051497888117, Reg_Loss: 1.7476473601618636e-07, Lr: 0.004166666666666667\n",
      "Epochs: 300, Acc: 0.004, Loss: 0.1520809439775955, Data_Loss: 0.15208071690951028, Reg_Loss: 2.2706808522343636e-07, Lr: 0.003846153846153846\n",
      "Epochs: 400, Acc: 0.004, Loss: 0.14702831979945896, Data_Loss: 0.14702800307179442, Reg_Loss: 3.167276645399397e-07, Lr: 0.0035714285714285718\n",
      "Epochs: 500, Acc: 0.005, Loss: 0.14236236008163689, Data_Loss: 0.14236192465695569, Reg_Loss: 4.3542468119994737e-07, Lr: 0.003333333333333333\n",
      "Epochs: 600, Acc: 0.005, Loss: 0.1395535464786457, Data_Loss: 0.13955300196052978, Reg_Loss: 5.445181159302592e-07, Lr: 0.003125\n",
      "Epochs: 700, Acc: 0.005, Loss: 0.13826966418919315, Data_Loss: 0.13826903713801417, Reg_Loss: 6.270511789807642e-07, Lr: 0.002941176470588235\n",
      "Epochs: 800, Acc: 0.005, Loss: 0.13770986583205508, Data_Loss: 0.13770918123617998, Reg_Loss: 6.84595875100058e-07, Lr: 0.002777777777777778\n",
      "Epochs: 900, Acc: 0.01, Loss: 0.13749938114472893, Data_Loss: 0.1374986600569137, Reg_Loss: 7.210878152363875e-07, Lr: 0.002631578947368421\n",
      "Epochs: 1000, Acc: 0.03, Loss: 0.1374144793283443, Data_Loss: 0.13741373342252186, Reg_Loss: 7.459058224412729e-07, Lr: 0.0025\n",
      "Epochs: 1100, Acc: 0.042, Loss: 0.13733392744535985, Data_Loss: 0.13733316344269605, Reg_Loss: 7.640026638000563e-07, Lr: 0.0023809523809523807\n",
      "Epochs: 1200, Acc: 0.05, Loss: 0.13729315523722027, Data_Loss: 0.13729237663392643, Reg_Loss: 7.786032938383869e-07, Lr: 0.0022727272727272726\n",
      "Epochs: 1300, Acc: 0.057, Loss: 0.13726642855141905, Data_Loss: 0.13726563801009775, Reg_Loss: 7.905413212938583e-07, Lr: 0.0021739130434782613\n",
      "Epochs: 1400, Acc: 0.063, Loss: 0.13724570671113304, Data_Loss: 0.13724490617173649, Reg_Loss: 8.005393965504481e-07, Lr: 0.0020833333333333333\n",
      "Epochs: 1500, Acc: 0.069, Loss: 0.13723252781220038, Data_Loss: 0.13723171883445515, Reg_Loss: 8.089777452369162e-07, Lr: 0.002\n",
      "Epochs: 1600, Acc: 0.074, Loss: 0.137220771487623, Data_Loss: 0.1372199552693336, Reg_Loss: 8.162182894011494e-07, Lr: 0.001923076923076923\n",
      "Epochs: 1700, Acc: 0.078, Loss: 0.1372138828868642, Data_Loss: 0.13721306030262348, Reg_Loss: 8.225842407227901e-07, Lr: 0.0018518518518518517\n",
      "Epochs: 1800, Acc: 0.082, Loss: 0.13720662216972954, Data_Loss: 0.13720579416734466, Reg_Loss: 8.280023848783458e-07, Lr: 0.0017857142857142859\n",
      "Epochs: 1900, Acc: 0.083, Loss: 0.13723767271768084, Data_Loss: 0.1372368396953968, Reg_Loss: 8.330222840413626e-07, Lr: 0.0017241379310344827\n",
      "Epochs: 2000, Acc: 0.089, Loss: 0.13719798691114835, Data_Loss: 0.13719714961389815, Reg_Loss: 8.372972502002085e-07, Lr: 0.0016666666666666666\n",
      "Epochs: 2100, Acc: 0.092, Loss: 0.13719590636182646, Data_Loss: 0.13719506509741722, Reg_Loss: 8.412644092459232e-07, Lr: 0.0016129032258064516\n",
      "Epochs: 2200, Acc: 0.095, Loss: 0.13719238446886714, Data_Loss: 0.13719153974223894, Reg_Loss: 8.447266282018973e-07, Lr: 0.0015625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m fig, ax = plt.subplots()\n\u001b[32m      9\u001b[39m fig.show()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43max\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mModel.fit\u001b[39m\u001b[34m(self, X, y, fig, ax)\u001b[39m\n\u001b[32m    105\u001b[39m     n = \u001b[32m0\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m reversed_dense_layer[n:]:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     dvalues = \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.pre_update_params()\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dense_layers:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mDense.backward\u001b[39m\u001b[34m(self, dvalues, *args)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, dvalues, *args):\n\u001b[32m     43\u001b[39m     dvalues = \u001b[38;5;28mself\u001b[39m.activation.backward(dvalues, *args)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28mself\u001b[39m.dweights = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mself\u001b[39m.dbiases = np.sum(dvalues, axis=\u001b[32m0\u001b[39m, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.L1_weight_regularizer > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.13/site-packages/nnfs/core.py:22\u001b[39m, in \u001b[36minit.<locals>.dot\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdot\u001b[39m(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43morig_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfloat64\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfloat32\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "X, y = sine_data()\n",
    "model = Model(12000)\n",
    "model.add_dense(1, 124, ReLU, L2_weight_regularizer=5e-8, L2_bias_regularizer=5e-8)\n",
    "# model.add_dropout()\n",
    "model.add_dense(124, 1, Linear)\n",
    "model.add_loss(MSE)\n",
    "model.add_optimizer(Adam, learning_rate=0.005, decay=1e-3)\n",
    "fig, ax = plt.subplots()\n",
    "fig.show()\n",
    "model.fit(X, y,fig,ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8f184-db95-42d7-b156-6db161304009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X = np.array([[0.7, 0.1, 0.21], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]])\n",
    "# y = np.array([0, 1, 1])\n",
    "\n",
    "# softmax_loss = Softmax_Activation_Categorical_Cross_Entropy_Loss()\n",
    "# print(X, y)\n",
    "# softmax_loss.backward(X, y)\n",
    "# dvalues1 = softmax_loss.dinputs\n",
    "\n",
    "# activation = Softmax_Activation()\n",
    "# activation.output = X\n",
    "# loss = Categorical_Cross_Entropy_Loss()\n",
    "# loss.backward(X, y)\n",
    "# print(loss.dinputs)\n",
    "# activation.backward(loss.dinputs)\n",
    "# dvalues2 = activation.dinputs\n",
    "\n",
    "# print(\"Gradients: combined loss and activation:\")\n",
    "# print(dvalues1)\n",
    "# print(\"Gradients: separate loss and activation:\")\n",
    "# print(dvalues2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813490bb-aaf6-4dc0-b0ba-9636da142899",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dense_1 = Dense(2, 64)\n",
    "# activation_1 = ReLU_Activation()\n",
    "\n",
    "# dense_2 = Dense(64, 3)\n",
    "# activation_2 = Softmax_Activation()\n",
    "\n",
    "# loss_function = Categorical_Cross_Entropy_Loss()\n",
    "\n",
    "# optimizer = SGD_Optimizer()\n",
    "# for epoch in range(1000):\n",
    "#     dense_1.forward(X)\n",
    "#     activation_1.forward(dense_1.output)\n",
    "#     dense_2.forward(activation_1.output)\n",
    "#     activation_2.forward(dense_2.output)\n",
    "#     loss = loss_function.calculate(activation_2.output, y)\n",
    "\n",
    "#     # print(\"Softmax Forward : \", activation_2.output)\n",
    "#     predictions = np.argmax(activation_2.output, axis=1)\n",
    "#     if len(y.shape) == 2:\n",
    "#         y = np.argmax(y, axis=1)\n",
    "\n",
    "#     accuracy = np.mean(predictions == y)\n",
    "#     # if not epoch % 100:\n",
    "#     print(f\"Epochs: {epoch}, Acc: {accuracy}, Loss: {loss}\")\n",
    "\n",
    "#     loss_function.backward(activation_2.output, y)\n",
    "#     # print(loss_function.dinputs.shape)\n",
    "#     activation_2.backward(loss_function.dinputs)\n",
    "#     # print(\"Softmax backward : \", activation_2.dinputs)\n",
    "\n",
    "#     dense_2.backward(activation_2.dinputs)\n",
    "#     activation_1.backward(dense_2.dinputs)\n",
    "#     dense_1.backward(activation_1.dinputs)\n",
    "\n",
    "#     optimizer.update_params(dense_1)\n",
    "#     # print(dense_1.weights)\n",
    "#     optimizer.update_params(dense_2)\n",
    "#     # print(dense_2.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c25da-293c-4200-ac1d-5eb7df19986d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dense_1 = Dense(2, 64)\n",
    "# activation_1 = ReLU_Activation()\n",
    "\n",
    "# dense_2 = Dense(64, 3)\n",
    "# activation_2 = Softmax_Activation_Categorical_Cross_Entropy_Loss()\n",
    "\n",
    "# sgd = SGD_Optimizer()\n",
    "# for epoch in range(20):\n",
    "#     dense_1.forward(X)\n",
    "#     activation_1.forward(dense_1.output)\n",
    "#     dense_2.forward(activation_1.output)\n",
    "\n",
    "#     loss = activation_2.forward(dense_2.output, y)\n",
    "\n",
    "#     # print(\"Softmax Forward : \", activation_2.output)\n",
    "\n",
    "#     predictions = np.argmax(activation_2.output, axis=1)\n",
    "#     if len(y.shape) == 2:\n",
    "#         y = np.argmax(y, axis=1)\n",
    "\n",
    "#     accuracy = np.mean(predictions == y)\n",
    "#     # if not epoch % 100:\n",
    "#     print(f\"Epochs: {epoch}, Acc: {accuracy}, Loss: {loss}\")\n",
    "\n",
    "#     activation_2.backward(activation_2.output, y)\n",
    "#     print(\"Softmax backward : \", activation_2.dinputs)\n",
    "\n",
    "#     dense_2.backward(activation_2.dinputs)\n",
    "#     activation_1.backward(dense_2.dinputs)\n",
    "#     dense_1.backward(activation_1.dinputs)\n",
    "\n",
    "#     sgd.update_params(dense_1)\n",
    "#     # print(dense_1.weights)\n",
    "#     sgd.update_params(dense_2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2084a7e9-df0c-4847-9d30-ddc3f0e8dc59",
   "metadata": {
    "tags": []
   },
   "source": [
    "h = 0.02  # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh using the model.\n",
    "# Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "dense_1.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "activation_1.forward(dense_1.output)\n",
    "dense_2.forward(activation_1.output)\n",
    "activation_2.activation.forward(dense_2.output)\n",
    "Z = np.argmax(activation_2.activation.output, axis=1)\n",
    "\n",
    "print(Z)\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "dense_1.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "activation_1.forward(dense_1.output)\n",
    "dense_2.forward(activation_1.output)\n",
    "activation_2.activation.forward(dense_2.output)\n",
    "Z = np.argmax(activation_2.activation.output, axis=1).reshape(xx.shape)\n",
    "\n",
    "print(Z.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "098c161b-82be-41cd-938a-367b0acf3452",
   "metadata": {
    "tags": []
   },
   "source": [
    "dense_1.forward(X)\n",
    "activation_1.forward(dense_1.output)\n",
    "dense_2.forward(activation_1.output)\n",
    "loss = activation_2.forward(dense_2.output, y)\n",
    "\n",
    "predictions = np.argmax(activation_2.output, axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93703f67-089c-4569-b9ad-08322cb2ef0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "def plot_decision_boundaries(X, y, model_class, **model_params):\n",
    "    \"\"\"Function to plot the decision boundaries of a classification model.\n",
    "    This uses just the first two columns of the data for fitting\n",
    "    the model as we need to find the predicted value for every point in\n",
    "    scatter plot.\n",
    "\n",
    "    One possible improvement could be to use all columns fot fitting\n",
    "    and using the first 2 columns and median of all other columns\n",
    "    for predicting.\n",
    "\n",
    "    Adopted from:\n",
    "    http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html\n",
    "    http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html\n",
    "    \"\"\"\n",
    "    reduced_data = X[:, :2]\n",
    "    model = model_class(**model_params)\n",
    "    model.fit(reduced_data, y)\n",
    "\n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = 0.02  # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    print(xx, yy)\n",
    "    # Obtain labels for each point in mesh using the model.\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    print(np.c_[xx.ravel(), yy.ravel()])\n",
    "    print(type(Z))\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    print(Z.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "049cd7b9-a05c-43d3-bb2b-ff03b0e2cd40",
   "metadata": {
    "tags": []
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "# X = iris.data[:, [0, 2]]\n",
    "# y = iris.target\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Training a classifier\n",
    "svm = SVC(C=0.5, kernel=\"linear\")\n",
    "svm.fit(X, y)\n",
    "\n",
    "# Plotting decision regions\n",
    "plot_decision_boundaries(X, y, SVC, C=0.5, kernel=\"linear\")\n",
    "\n",
    "# Adding axes annotations\n",
    "plt.xlabel(\"sepal length [cm]\")\n",
    "plt.ylabel(\"petal length [cm]\")\n",
    "plt.title(\"SVM on Iris\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a90b047-b110-426f-97c4-d1450bcf1f11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
